---
title: "Forecasting and Time Series Methods Notes 7"
author: "[Xiaorui Zhu](https://homepages.uc.edu/~zhuxr/) (zhuxr@mail.uc.edu)"
output: 
  html_document:
    code_folding: show
    fig_caption: yes
    highlight: tango
    number_sections: yes
    theme: readable
    toc: yes
editor_options: 
  chunk_output_type: console
---
<style type="text/css">
body{ /* Normal  */
      font-size: 20px;
  }
code.r{ /* Code block */
    font-size: 16px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 16px;
}
h1 { /* Header 1 */
  color: DarkBlue;
}
h2 { /* Header 2 */
  color: DarkBlue;
}
h3 { /* Header 3 */
  color: DarkBlue;
}
</style>



```{r, warning=FALSE,message=FALSE, results = 'hide'}
# Install necessary packages
list_packages <- c("AER", "dynlm", "fpp", "fpp2", 
                   "forecast", "readxl", "stargazer", "scales",
                   "quantmod", "urca", "vars", "tseries", "sarima")
new_packages <- list_packages[!(list_packages %in% installed.packages()[,"Package"])]
if(length(new_packages)) install.packages(new_packages)

# Load necessary packages
lapply(list_packages, require, character.only = TRUE)
```

# Seasonal Data

A time series model exhibits periodic  behavior with period $s$, if similarities in the time series data occurs after a $s$ time interval.

A particular kind of periodic behavior is seasonal behavior in which there are similar "with-in-year" patterns from year to year, e.g., toy sales, airlines usage.

If the basic period is a month, then the period is $s=12$.  If the data is collected by quarters, then the period is s=4, and there are other possibilities where there can be more than one periodicity, e.g., daily observed data may weakly, monthly, quarterly, yearly patterns.

ARIMA models can be employed successfully to fit and forecast seasonal time series data.  The techniques require no new development.

One useful representation of seasonal data is as data from a two way table.  Here is an example


Year $\downarrow$ + month $\to$ | Jan    | Feb    | Mar    | ... ... ... ... | Dec    |
------+--------+--------+--------+-----------------+---------
$1$     | $Z_1$    | $Z_2$    | $Z_3$    | ... ... ... ... | $Z_{12}$ |
$2$     | $Z_{13}$ | $Z_{14}$ | $Z_{15}$ | ... ... ... ... | $Z_{24}$ |
$3$     | $Z_{25}$ | $Z_{26}$ | $Z_{27}$ | ... ... ... ... | $Z_{36}$ |
  ... | ... | ... | ... | ... ... ... ... | ... |
$r$ | $Z_{12(r-1)+1}$ | $Z_{12(r-1)+2}$ | $Z_{12(r-1)+3}$ | ... ... ... ... | $Z_{12r}$ |

Within each year (row), the data represents a type of month to month pattern.

Within each month (column), the data represents a type of year to year time pattern.

This is the basic for one useful method for modeling seasonal time series data by multiplicative models.  This model attempts to model these two types of patterns separately and then combine both features afterwards.  

More specifically, we will build two models: one ARIMA for the column and one ARIMA for the row.  We will combine these two models to have **seasonal ARIMA**.

The assumptions and notations

$s$-backward operator: $B^s$

$B^s Z_t = Z_{t-s}$

$s$-difference operator: $\nabla_s = 1-B^s$

$\nabla_s Z_t = Z_t - Z_{t-s}$

$\nabla_s^D Z_t = (1-B^s)^D Z_t$ is the $D$-th difference operator on the between period time series data.

# Pure Seasonal ARIMA Models
Denote the $P$ AR parameters of this model as $\Phi_s$, $\Phi_{2s}$,..., and $\Phi_{Ps}$, and the autoregressive polynomial is defined as $$\Phi_P(B^s)=1-\Phi_s B^s - \Phi_{2s} B^{2s} - ... - \Phi_{Ps} B^{Ps}.$$

Denote the $Q$ MA parameters of this model as $\Theta_s$, $\Theta_{2s}$,..., and $\Theta_{Qs}$, and the moving average polynomial is defined as $$\Theta_Q(B^s)=1-\Theta_s B^s - \Theta_{2s} B^{2s} - ... - \Theta_{Qs} B^{Qs}.$$

Let $\nabla_s^D = (1-B^s)^D$, then the between period model, i.e., the pure seasonal ARIMA model, can be written as 

$$
\Phi_P(B^s) \nabla_s^D \tilde{Z}_t = \Theta_Q(B^s)a_t
$$



## SARMA$(p,q)_s$

When $D=0$, we have

$$
\tilde{Z}_t = \Phi_s \tilde{Z}_{t-s} + \Phi_{2s} \tilde{Z}_{t-2s} ... + \Phi_{Ps} \tilde{Z}_{t-Ps} + a_t - \Theta_s a_{t-s} - \Theta_{2s} a_{t-2s} - ... - \Theta_{2Q} a_{t-2Q}
$$



## SAR$(1)_{12}$

$$
\tilde{Z}_t = \Phi_{12} \tilde{Z}_{t-12} + a_t
$$
```{r, fig.width=9,fig.height=3}
n=200000
ts1 <- sim_sarima(n,model=list(sar=0.8, nseasons=12, sigma2 = 1))
par(mfrow=c(1,3))
acf(ts1,ylim=c(-1,1))
pacf(ts1,ylim=c(-1,1))
plot(ts1[1:100],type="l")
```

## SMA$(1)_{12}$

$$
\tilde{Z}_t = a_t - \Theta_{12} a_{t-12}
$$

```{r, fig.width=9,fig.height=3}
n=200000
ts1 <- sim_sarima(n,model=list(sma=0.8, nseasons=12, sigma2 = 1))
par(mfrow=c(1,3))
acf(ts1,ylim=c(-1,1))
pacf(ts1,ylim=c(-1,1))
plot(ts1[1:100],type="l")
```

# Seasonal ARIMA Models

These pure seasonal models have so far treated the period to period process as an independent process, i.e., no correlation at the lag other than the integer multiple of $s$, which is a deficiency.

Therefore, to take consideration of all the dynamics (both between period and within period), we further assume the error component $a_t$ are correlated.  In fact, we model these $a_t$ as another ARIMA(p,d,q), i.e., $a_t$ satisfies

$$
(1-\phi_1B-...-\phi_p B^p)(1-B)^d a_t = (1-\theta_1 B - ... - \theta_q B^q) \epsilon_t
$$
where $\epsilon_t$ is white noise.  It can be written as

$$
\phi(B)(1-B)^d a_t = \theta(B) \epsilon_t
$$

Or equivalently,

$$
a_t = \phi^{-1}(B)(1-B)^{-d} \theta(B) \epsilon_t
$$

Substitute it into the pure seasonal model $\Phi_P(B^s)\nabla_s^D \tilde{Z}_t = \Theta_Q(B^s)a_t$, we have the **seasonal ARIMA model** as


$$
\Phi_P(B^s)\nabla_s^D \tilde{Z}_t = \Theta_Q(B^s)\phi^{-1}(B)(1-B)^{-d} \theta(B) \epsilon_t
$$

which is equivalent to
 
$$
\Phi_P(B^s) \phi(B) (1-B^s)^D (1-B)^{d} \tilde{Z}_t = \Theta_Q (B^s) \theta(B) \epsilon_t
$$

This is called the Box-Jenkins Seasonal Multiplicative Model of order $(p,d,q)\times (P,D,Q)_s$.

The nonseasonal part (within period) is governed by $(p,d,q)$ with AR in $p$ and MA in $q$ and number of differencing in $d$, $Z_t-Z_{t-1}$.  The seasonal part (between period) is governed by $(P,D,Q)_s$ with seasonal AR in $P$ and seasonal MA in $Q$ and the number of seasonal differencing in $D$, $Z_t-Z_{t-s}$.


## SARIMA$(0,0,1) \times (1,0,0)_{12}$

$$
(1-\Phi_{12} B^{12}) \tilde{Z}_t = a_t - \Theta_{12} a_{t-12}
$$

```{r, fig.width=9,fig.height=3}
n=200000
ts1 <- sim_sarima(n,model=list(ma=0.4, sar=0.8, nseasons=12, sigma2 = 1))
par(mfrow=c(1,3))
acf(ts1,ylim=c(-1,1))
pacf(ts1,ylim=c(-1,1))
plot(ts1[1:100],type="l")
```


## SARIMA$(1,0,0) \times (1,0,0)_{12}$

$$
(1-\phi_{1} B) (1-\Phi_{12} B^{12})\tilde{Z}_t = a_t
$$

```{r, fig.width=9,fig.height=3}
n=200000
ts1 <- sim_sarima(n,model=list(ar=0.4, sar=0.8, nseasons=12, sigma2 = 1))
par(mfrow=c(1,3))
acf(ts1,ylim=c(-1,1))
pacf(ts1,ylim=c(-1,1))
plot(ts1[1:100],type="l")
```

It can be proved that the ACF is symmetric around period intervals, i.e., $\rho_{s+1}=\rho_{s-1}$.

## SARIMA$(0,0,1) \times (0,0,1)_{12}$

$$
\tilde{Z}_t = (1-\Theta_{12} B^{12})(1-\theta_{1}B)a_t
$$

```{r, fig.width=9,fig.height=3}
n=200000
ts1 <- sim_sarima(n,model=list(ma=0.4, sma=0.8, nseasons=12, sigma2 = 1))
par(mfrow=c(1,3))
acf(ts1,ylim=c(-1,1))
pacf(ts1,ylim=c(-1,1))
plot(ts1[1:100],type="l")
```

## SARIMA$(1,0,0) \times (0,0,1)_{12}$

$$
\tilde{Z}_t = a_t - \Theta_s a_{t-s}
$$

```{r, fig.width=9,fig.height=3}
n=200000
ts1 <- sim_sarima(n,model=list(ar=0.4, sma=0.8, nseasons=12, sigma2 = 1))
par(mfrow=c(1,3))
acf(ts1,ylim=c(-1,1))
pacf(ts1,ylim=c(-1,1))
plot(ts1[1:100],type="l")
```

## SARIMA$(0,1,0) \times (1,0,0)_{12}$

$$
(1-\phi_1 B )(1-B^{12})\tilde{Z}_t = a_t
$$

```{r, fig.width=9,fig.height=3}
n=200000
ts1 <- sim_sarima(n,model=list(sar=0, ar=0.5, siorder=1, nseasons=12, sigma2 = 1))
par(mfrow=c(1,3))
acf(ts1,ylim=c(-1,1))
pacf(ts1,ylim=c(-1,1))
plot(ts1[1:100],type="l")
```

## SARIMA$(0,1,0) \times (0,1,0)_{12}$

$$
(1- B )(1-B^{12})\tilde{Z}_t = a_t
$$

```{r, fig.width=9,fig.height=3}
n=200000
ts1 <- sim_sarima(n,model=list(iorder=1, siorder=1, nseasons=12, sigma2 = 1))
par(mfrow=c(1,3))
acf(ts1,ylim=c(-1,1))
pacf(ts1,ylim=c(-1,1))
plot(ts1[1:100],type="l")
```


## SARIMA$(1,0,0) \times (1,0,0)_{12}$

$$
(1- \phi_1 B )(1- \Phi_{12} B^{12})\tilde{Z}_t = a_t
$$

```{r, fig.width=9,fig.height=3}
n=200000
ts1 <- sim_sarima(n,model=list(sar=0.9,ar=0.9, nseasons=12, sigma2 = 1))
par(mfrow=c(1,3))
acf(ts1,ylim=c(-1,1))
pacf(ts1,ylim=c(-1,1))
plot(ts1[1:100],type="l")
```

These examples give us some intuition of the ACF of seasonal ARIMA.

# Diagnostic Checking, Forecast, and Stationarity

Seasonal ARIMA presents no new problems in terms of diagnostic checking.  We simply check adequacy of the a ARIMA model.

Forecast also presents no new challenges.

The condition of stationarity and invertibility for seasonal ARIMA is a direct extension of regular ARIMA.  Namely, the seasonal ARIMA is stationary if the roots of both $\phi_p(B)=0$ and $\Phi_P(B^s)=0$ lie outside of unit circle.  The seasonal ARIMA is invertible if the roots of both $\theta_q(B)=0$ and $\Theta_Q(B^s)=0$ lie outside of unit circle.   

# Real Data Examples

**Case 9: air-carrier freight**
```{r}
case9=ts(scan("case9.txt"), start=c(1969,1), end=c(1978,12), frequency=12)
case9
case9 %>% ggtsdisplay(lag.max=40)
```

Visual check finds the variance increases as time goes by, therefore, we use box-cox transformation with lambda=0, i.e., log transformation.

```{r}
case9 %>% log() %>% ggtsdisplay(lag.max=40)
```

The variance is stablized.  Because of the ACF, we consider it to be nonstationary and take a 1st order difference.

```{r}
case9 %>% log() %>% diff() %>% ggtsdisplay(lag.max=40)
```

The ACF at lags 12, 24, 36 are still decreasing slowly, therefore, we take a seasonal 1st order difference.

```{r}
case9 %>% log() %>% diff() %>% diff(lag=12) %>% ggtsdisplay(lag.max=40)
case9 %>% log() %>% diff() %>% diff(lag=12) %>% adf.test()
```

The ACF now seems to be stationary.  Perform ADF test for stationarity also confirms it.

The spike at lag 1 suggests a MA(1).  The spike at lag 12 suggests a seasonal MA(1).  Therefore, we fit a ARIMA(0,0,1)(0,0,1)[12] to the both seasonally and nonseasonally differenced series.

```{r}
(case9 %>% log() %>% diff() %>% diff(lag=12) %>% 
  Arima(order=c(0,0,1), seasonal=c(0,0,1),include.constant = FALSE))
```

This model is equivalent to fit a ARIMA(0,1,1)(0,1,1)[12] to the original series with a log transformation.  We further check its residuals.

```{r}
(fit <- Arima(case9, order=c(0,1,1), seasonal=c(0,1,1),include.constant = FALSE,lambda=0))
checkresiduals(fit)
```

The residuals looks all right. Finally, we take our selected model to forecast the next 3 years.

```{r}
fit %>% forecast(h=36) %>% autoplot()
```

Note that we can compare the model with `auto.arima` and the suggested model is slightly different.

```{r}
auto.arima(log(case9))
```

**European quarterly retail trade:** We will describe the seasonal ARIMA modelling procedure using quarterly European retail trade data from 1996 to 2011.

```{r}
data("euretail")
euretail
autoplot(euretail) + ylab("Retail index") + xlab("Year")
euretail %>% ggtsdisplay(lag.max=40)
euretail %>% adf.test()
euretail %>% diff(lag=4) %>% ggtsdisplay(lag.max=40)
euretail %>% diff(lag=4) %>% diff() %>% ggtsdisplay(lag.max=40)
euretail %>% diff(lag=4) %>% diff() %>% adf.test()
```

The significant spike at lag 1 in the ACF suggests a non-seasonal MA(1) component, and the significant spike at lag 4 in the ACF suggests a seasonal MA(1) component. Consequently, we begin with an ARIMA(0,1,1)(0,1,1)[4] model
  
```{r}
euretail %>%
  Arima(order=c(0,1,1), seasonal=c(0,1,1)) %>%
  residuals() %>% ggtsdisplay()
```

Both the ACF and PACF show significant spikes at lag 2, and almost significant spikes at lag 3, indicating that some additional non-seasonal terms need to be included in the model.

```{r}
fit3 <- Arima(euretail, order=c(0,1,3), seasonal=c(0,1,1))
checkresiduals(fit3)
```

We tried other models with AR terms as well, but none that gave a smaller AICc value.

we now have a seasonal ARIMA model that passes the required checks and is ready for forecasting.

```{r}
fit3 %>% forecast(h=12) %>% autoplot()
```

We could have used auto.arima() to do most of this work for us

```{r}
auto.arima(euretail)
```

The `auto.arima()` function uses `nsdiffs()` to determine  $D$  (the number of seasonal differences to use), and `ndiffs()` to determine $d$  (the number of ordinary differences to use).

```{r}
euretail %>% nsdiffs()
euretail %>% ndiffs()
```

**Example: Corticosteroid drug sales in Australia**

```{r}
lh02 <- log(h02)
cbind("H02 sales (million scripts)" = h02,
      "Log H02 sales"=lh02) %>%
  autoplot(facets=TRUE) + xlab("Year") + ylab("")
```

The data are strongly seasonal and obviously non-stationary, so seasonal differencing will be used. The seasonally differenced data are shown below. It is not clear at this point whether we should do another difference or not. We decide not to, but the choice is not obvious.

```{r}
lh02 %>% diff(lag=12) %>%
  ggtsdisplay(xlab="Year",
    main="Seasonally differenced H02 scripts")
```

In the plots of the seasonally differenced data, there are spikes in the PACF at lags 12 and 24, but nothing at seasonal lags in the ACF. This may be suggestive of a seasonal AR(2) term.

In the non-seasonal lags, there are three significant spikes in the PACF, suggesting a possible AR(3) term. The pattern in the ACF is not indicative of any simple model.

We fit this model, along with some variations on it

```{r}
(fit <- Arima(h02, order=c(3,0,0), seasonal=c(2,1,0),
  lambda=0))

(fit <- Arima(h02, order=c(3,0,1), seasonal=c(0,1,2),
  lambda=0))

checkresiduals(fit, lag=36)
```

The model fails the Ljung-Box test.

Next we will try using the automatic ARIMA algorithm. Running `auto.arima()` with all arguments left at their default values

```{r}
auto.arima(lh02)
```

However, the model still fails the Ljung-Box test for 36 lags

**Sometimes it is just not possible to find a model that passes all of the tests.**

```{r}
h02 %>%
  Arima(order=c(3,0,1), seasonal=c(0,1,2), lambda=0) %>%
  forecast() %>%
  autoplot() +
    ylab("H02 sales (million scripts)") + xlab("Year")

```


**Case 10** After tax profits measured in cents per dollar of sales for all US manufacturing companies. Quarterly data from 1953-1972.

```{r}
case10=ts(scan("case10.txt"),start=c(1953,1), end=c(1972,4),frequency=4)
case10 %>% ggtsdisplay(lag.max=40)
```

From the ACF and PACF, we don't see anything similar to nonstationary time series.  **However, we take 1st order difference anyway.**

**This is important because taking 1st order difference should be done routinely whenever the data might have seasonal or other periodic variation, even if the nonseasonal difference is not needed.  Often the nature of a seasonal pattern emerges more clearly  in the acf of the differenced series.**

```{r}
case10 %>% diff() %>% ggtsdisplay(lag.max=40)
```

The ACF at lags 4 and 8 suggest a seasonal MA(2).  Note that the PACF at lags 4, 8, 12, and 16 are expoentially decaying.  Therefore, maybe we should fit an SARIMA(1,0,0)(0,0,2)[4].  Note that we didn't need to take the first order difference, but took it anyway. That's why it is AR(1) not $d=1$.

```
(case10 %>% Arima(order=c(1,0,0), seasonal=c(0,0,2)))
case10 %>% Arima(order=c(1,0,0), seasonal=c(0,0,2)) %>% checkresiduals()
```

It seems the SARIMA(1,0,0)(0,0,2)[4] fits the data well.

**Even if we miss the 1st order difference at the begining, we can still identify this model.**  Here is another way.  Suppose we start with AR(1) based on the ACF and PACF.

```{r}
case10 %>% Arima(order=c(1,0,0), seasonal=c(0,0,0)) %>% residuals() %>% ggtsdisplay(lag.max=20)
```

From the residuals ACF and PACF, it suggest a seasonal MA(2).  So we add it to the original model.

```{r}
case10 %>% Arima(order=c(1,0,0), seasonal=c(0,0,2)) %>% residuals() %>% ggtsdisplay(lag.max=20)
```
Finally, we take this model and perform forecasting.

```{r}
(fit <- case10 %>% Arima(order=c(1,0,0), seasonal=c(0,0,2)))
fit %>% forecast(h=20) %>% autoplot()
```